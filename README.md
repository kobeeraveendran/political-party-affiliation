# Political Party Affiliation using Modern Natural Language Processing

## Motivation

This project is based on a similar work I encountered in the MachineLearning subreddit. That work attempts to predict the political alignment of a Reddit user by taking their active subreddits as input features to a logistic regression model. The outputs are labels drawn from another subreddit, `r/PoliticalCompassMemes`, in which user flairs serve as labels denoting their specific political leaning.

* Link to the original post in `r/MachineLearning`: [[P] Predict your political leaning from your reddit comment history!](https://www.reddit.com/r/MachineLearning/comments/jdeyp9/p_predict_your_political_leaning_from_your_reddit/)

* Link to the author's Github repository: [reddit-stance-classifier](https://github.com/0xTiger/reddit-stance-classifier)

## Environment Setup

### Scraping and Simple Model Experiments

Included are the two `conda` environments I used for training and evaluating all of the methods from the paper. To scrape data from Reddit, and to run the experiments involving multinomial naive Bayes, linear SVMs, AdaBoost, and GRUs, you will need the `nlp` environment. I've provided two versions of this environment: one that exactly matches what I used (down to platform-specific versions) and one with only packages I've installed explicitly through conda, without platform-specific versioning. It's recommended to start by creating an environment with the concise version like so:

```
conda create -f nlp_concise.yml
conda activate nlp
```

Since the concise version does not keep track of packages installed using `pip`, you may encounter issues when running. If you do, simply run `pip install <missing_package_name_here>` in the environment to fix this. Feel free to look at my verbose environment in `nlp_verbose.yml` to see what versions I used. Or, if you are running on a Linux machine, chances are the verbose environment may work out of the box for you:

```conda create -f nlp_verbose.yml
conda activate nlp
```

### Transformer Experiments

To run experiments on the different transformers,  you will instead need the `simple-t` environment. In a similar fashion to the `nlp` environment, this can be created on your system using the concise version:

```
conda create -f simplet-concise.yml
conda activate simple-t
```

or the verbose version:

```
conda create -f simplet-verbose.yml
conda activate simple-t
```

## Dataset Generation

The dataset is generated by scraping Reddit using the Python Reddit API Wrapper (PRAW). First, you will need to set up a Reddit account and a new Reddit bot to perform the scraping. You can do this by going [here](https://ssl.reddit.com/prefs/apps/) and registering a new Reddit bot. Once you've registered the bot, fill in its information and your Reddit login credentials in `example_config.py`, then rename `example_config.py` to `config.py`. This is purely to enable authentication for your bot to allow it to interact with Reddit and scrape comments.

You can then run my scraping script. An example command is:

```bash
python subreddit_scrape.py --subreddit conservative --batch_size 1000 --score_threshold 5
```

Args:

* `subreddit`: the name of the subreddit you wish to scrape, case insensitive in my experience
* `batch_size`: the total number of comments to be scraped that match the criteria specified in the paper
* `score_threshold`: lower bound on the score required for a comment to be recorded (for example, in this command, comments with a score *greater than* 5 will be recorded)

The scraped comments will be in `.txt` files matching the name of the subreddit they were collected from. You may concatenate these if the subreddits share the same political alignment.

Below are the series of regular expressions I used (in order) to clear out URLs from the data (multiple are needed because of how users tend to use the Reddit markdown syntax for including links):

* `\(http\S+\)`
* `\[http\S+\]`
* `http\S+`


## Running Experiments

You may now safely be able to run the experiments. You can run an experiment by simply using the name of the model (excluding transformer models; see below):

```bash
conda activate nlp              # activate the nlp conda env

python multinomial_nb.py        # multinomial naive bayes experiment
python linear_svm.py            # linear SVM experiment
python adaboost.py              # AdaBoost experiment
python gru.py                   # GRU experiment
```

To run experiments on the transformer models, switch to the `simple-t` env and run the transformers code as follows:

```bash
conda activate simple-t                         # activate the simple-t conda env

python eval_transformers.py --model roberta     # run experiments on the DistilRoBERTa model
```

`eval_transformers` takes a single argument specifying the model to use. Your options are BERT, XLNet, and DistilRoBERTa, which can be specified by supplying `--model` with one of the following:

* `bert`
* `xlnet`
* `roberta`

If an argument is invalid or no argument is supplied, `bert` is chosen by default.